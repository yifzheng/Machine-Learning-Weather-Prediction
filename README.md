# Machine Learning Weather Prediction Report
## By Yifeng Zheng

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Weather prediction has always been an arduous task mankind had before machines appeared, alleviating the necessity of data calculations. One might have gathered weather data such as temperature, humidity, etc. and attempted to predict if tomorrow will rain or not but such a feat is almost unsurmountable. The machine learning question to be answered is derived from the observable weather data from the past 10 years in Australia and whether a predictive classifier with enough accuracy can be created to predict next day rain. This is an important question that needs to be answered because it decides whether there is a next step in meteorology. With more accurate rain predictions, we have more stable farming routines, improve transportation safety, event planning, etc. With this predictive classifier, farmers can know the best time for crops and the optimal farming practices that could prevent costly decisions. Transportation has improved safety as weather predictions provide the data and what weather conditions to look out for before any form of transportation starts. In this project, rain prediction is especially useful for aircraft as there are a lot of risks in flying in rainy weather. Overall, solving this machine learning question protects human lives and property, safety, and economic prosperity.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The dataset used in this machine learning project is called Australia Weather Data by Arunava KR. Chakaraborty found on Kaggle. This dataset contains about 10 years of daily weather observations from many locations in Australia. The total dataset has over 100,000 instances separated into training and test sets. However, after observing both datasets, I discovered that the test data set only has the inputs or features and no predictive labels. Therefore, I decided to use the training set which has about 100,000 instances as the main dataset for this project. In total, this dataset contains 21 features and 1 predictive label, RainTomorrow. The predictive label seems to be encoded in an ordinal scheme but whether the data values are 0/false or 1/true has no effect on the results of the model training and result predictions. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data preprocessing steps for the data are simple as it is relatively complete. There are several features in the data set that are irrelevant to the training such as “row ID” and have null or empty values for the majority of the instances in the dataset. For the empty columns, there isn’t enough data to process the data and insert the median value into the empty instances. Therefore, I removed 6 features from the data set. The next step of data preprocessing was to standardize the numerical features and encode the categorical features. Using two variables, I separated the numerical and categorical features. Next, using a pipeline, I transformed the dataset using a StandardScalar and OntHotEncoder and stored it in a variable. Afterward, I performed the previous two steps again but this time I transformed the dataset with fewer numerical features for testing if there are any irrelevant features in the data. Next using the unprepared dataset, I used two features to plot the data into a scatterplot using the label as the hue indicator. From this scatterplot, we can see that there are 2 major clusters of data and there is no definite linear line that can separate them into two classes. One might try but there will be many cases of misclassification so I predict that there might not be a high score in the classification report for linear classification such as logistic regression and clustering algorithms. The data is too spread out for clustering because there are only two classes and we need only two clusters. However, no single cluster can be put in the plot that can have a classification score high enough with low error. Additionally, the ratio of data for rain and no rain is disproportional making no rain to rain about 0.28 or 28%. The train, valid, and test set will hopefully maintain this ratio. To create the training, validation, and test set, I first did a train_test_split using training_prepared as X and the label feature “Rain Tomorrow” as y into 4 sets of data: X_t, X_test, y_t, y_test. Additionally, I used the label feature and created a binary classifier so every value of 0 is set to false and every value of 1 is set to true. For the train_test_split, the training set has 80% of the data and I used a random state value so every time the code is run, I get back the same data in each set of data. X_test and y_test would be used during the final evaluation with the best algorithm. X_t and y_t would be used to separate into X_train, X_valid, y_train, y_valid sets. After the preprocessing is finished, the X data has a shape of ([# of instances],62) and is stored in a sparse matrix.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This project is following a survey structure, so the machine algorithms used will be suited for classification tasks. From the linear model, I will be using Logistic Regression and SGDClassifier. From the neighbors category, I will use the KNeighbors Classifier. For support vector machines, I will use SVC. From the tree model, I will use Decision Tree Classifier and the ensemble model the Random Forest Classifier. These two are decision trees and Random Forest is used to test if it’ll perform better than the generic Decision Tree Classifier. For clustering algorithms, I will be using KMeans. Finally, I will create a neural network using the Sequential API for SoftMax classification. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The first experiment in this project was logistic regression and then I gathered the hyperparameters of the logistic model and performed grid search. The parameters chosen were penalty which was narrowed down to “l2” and “none” and C which had 0.001, 0.01, 0.1, 1, and 2. With accuracy as the scoring metric and each combination of the parameters cross-validated 3 times, grid search was performed and the best parameters for the logistic model after several cases of grid search was the penalty being “l2” and the C value 0.1, The l2 penalty tries to minimize the values of less important features and the C value is the same as in support vector machines which changes how lenient the model is towards misclassifications. From this value, there is more leniency for misclassifications. Using these parameters, a logistic model is trained again and the X_valid dataset was used to predict y, the label values, and the accuracy score and classification report of this model was returned. This model had an accuracy score of 0.8483771618099977 which is 0..85 rounded up. This value had no other data to compare to so I did not know if this value was good or not, but I expected a lower accuracy score because of the way the data was visualized in the scatterplot. Based on the classification report, logistic regression has an accuracy of 85%. This score seems low even though a grid search was performed to find the best hyperparameters that limit regularization. From the test, hyperparameter 'C' that has significance in SVMs for regularization has no effect on classification when the penalty is 'l2' after C > 1. Furthermore, this model demonstrates the limitations of Logistic Regression for this machine-learning problem because this algorithm is linear. The model assumes linearity between the features and the label when there is no direct relationship at all. Logistic Regression finds a probability by a definite linear line which isn't helpful when there are complex relationships and non-linearly separated data such as the features in this dataset. Consequently, the classification tasks for this dataset are not optimally fit for Logistic Regression. The second experiment performed with logistic regression is the test for irrelevant features. Using the other training sets created with fewer numerical features, I created a logistic model, trained the model, and predicted the data. Results from this model were unsatisfactory as the accuracy score was 0.82. The first irrelevant feature test removed the last two numerical features, and the values of the classification report remained the same while the test shown on the file removed about half of the numerical features. This implies that the number of numerical features after the removal of the features that are null or empty is necessary for the training of the models.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The second algorithm used was the KNeighbors Classifier. Similar to the logistic model, grid search was performed on the n_neighbors and weights hyperparameter. The weights are either distance or uniform but n_neighbors was the parameter that took the most time to narrow down. The initial grid search had n_neighbors with values from 1-10. After a value was returned by grid search, I slowly increased the range of the neighbors within a margin of 1. This led to the final parameters used for the grid search being 19, 20, and 21. The scoring metric was accuracy and each grid search was cross-validated 3 times. The results returned from the final search indicated that the best parameters were n_neighbors being 20 and weights being distance. I used these values to train a KNeighbors classifier and the resulting accuracy score of this model was 0.843875 and from the classification report, the accuracy was 0.84, the macro average was 0.73, and the weighted average is 0.83. KNeighbors Classifier implements the k-nearest neighbors’ vote. The hyperparameters used for regularization are n_neighbors being 20 and weights being distance. With these parameters, each instance needs to find 20 of its nearest neighbors for reference and returns the distance from the instance. With the weight of the classifier being distance, there is an inverse relationship for the distance between the points making the instances closer to the sample have a higher influence. From the result shown by the classification report, the accuracy for this model is 0.84 or 84% which is 0.1 or 1% less than the logistic regression model. Furthermore, the macro average is 0.2 less, and the weighted avg 0.1 less than the logistic regression model. On average, this model performed worse than the logistic model.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The third algorithm used was the SGD Classifier. This model required 4 parameters for grid search but surprisingly needed the least time for selecting the best parameters. Similar to the previous two models, the parameters for grid search are the same and the resulting parameters of the search were alpha: 0.001, eta0: 1, learning_rate: optimal, and loss: log. The resulting accuracy score was 0.848140. From the hyperparameters set above, the SGDClassifier behaves as a logistic regressor. While it may be the same, the classification report indicates a better macro avg for precision and a worse macro avg for recall in the SGD model. However, while these few data points fluctuate, the performance for f1_score remains the same for both the SGD classifier and logistic regression model indicating that the stochastic gradient descent algorithm didn't have a great influence on the classification of labels.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SVM has a classifier with a hyperparameter C that is used as a regularization parameter. This value is inversely related to the strength of regularization. The greater the value of C, the more tolerated the cases of misclassification and vice versa for smaller values of C. There are also kernels used by this SVM which manipulates the data in higher dimensions without having to perform heavy calculations. In this project, the kernels to be used are rbf and poly. With the poly kernel, a parameter degree will have to be included for a grid search. With rbf, we need gamma as a parameter to control the curvature of the decision boundary. This algorithm was separated into two parts with each part testing an individual kernel. The first kernel for grid search was the rbf kernel and the hyperparameters necessary for this kernel was C and gamma. Through several cases of testing, the best parameters for this kernel were C = 1 and gamma = 0.1. With C being 1, the SVC is doing soft margin classification with more leniency for misclassifications and the gamma being 0.1 each instance has a large similarity radius making more points grouped together. Additionally, with a small gamma value, the effect of C affects the model just as it does in a linear model. This model demonstrates that any classification algorithm that requires a linear decision boundary is set to have an average accuracy score. As with logistic regression and SGD Classifier with the log loss function, the support vector classifier with the rbf kernel and parameters C=1 and gamma=0.1 has the same accuracy score of 0.85, specifically 0.8569059464581853. However, it performs slightly better rounding the accuracy to 0.86. With the poly kernel, we replace the gamma parameter with two new parameters: coef0 and degree. Coef0 is used to scale the data should the degree get too big and the resulting values become exponentially large. With grid search, several tests narrowed the parameters of C to 0.001 and 0.0001, coef0 and degree to 5 and 6. The resulting parameters from grid search became C = 0.001, coef0 = 5, degree = 5, kernel = poly. The accuracy of the model after being trained with these parameters and predicting the values of y was 0.857458738055753. This accuracy score is about 0.005 higher than the rbf kernel but the classification report for this model has further details that show the rbf kernel had better results. From the classification report, the poly kernel was 0.02 higher than the rbf kernel for the precision of true values, 0.02 lower for recall of true, and 0.01 smaller for f1_score. The macro and weighted average for f1_score is 0.01 lower than the rbf kernel so despite the accuracy score being higher, I conclude that the rbf kernel had a better performance on average. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For decision trees, there are two different algorithms used which are the generic decision tree classifier and the random forest classifier. A random forest is a meta estimator that fits several decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. I predicted that this classifier would have a similar performance compared to the generic decision tree. The grid search for the decision tree classifier was intensive as there were many parameters needed to tune. After 1,200+ fits of grid search, the optimal parameters for this model were criterion: Gini, max_depth: 6, min_samples_leaf: 1, min_samples_split: 3, and splitter: best. The accuracy score after training a new classifier from these parameters was 0.8434810076601121. However, results for the decision tree on this real-world data are not optimistic. Despite how quick the grid search for parameters was compared to previous models, accuracy is not as optimal as SVM. Recall is 0.02 or 0.01 less than the logistic model and SVM classifiers with rbf and poly kernels. Random forest was easier to grid search and after 4 iterations, I was able to narrow the parameters down to n_estimators: [400, 600], max_leaf_node: none, n_jobs: -1, and criterion: Gini. The best parameters from the final grid search were criterion = Gini, max_leaf_nodes = none, n_estimators = 400, and n_jobs = -1. The accuracy score for the model was 0.8538261075574508 which outperformed the logistic, kneighbors, and sgd classifier models. However, the classification report of this model indicates that it performs slightly worse on average compared to SVC with the rbf and poly kernel.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The algorithm that I had the least hope for in this entire project was the clustering algorithms. As discussed earlier, the data for the two classes, 1/true and 0/false, are too compact for a clustering algorithm to effectively decide on two nodes to classify the data. The hyperparameters chosen were n_clusters = 2 and init = k-means++. This allows the model to have two nodes as there are only two classes: true and false and k-means++ selects the initial clusters using sampling based on an empirical probability of the points’ contribution to the overall inertia. All that is needed to determine was the value of n_init which was narrowed down to 1, 5, 10, and 15. The resulting best parameters was n_init being 1. After the model was trained based on these parameters, the accuracy score was 0.5101476743267788 which was surprising as the model had about a 50% chance of classifying the data correctly. However, while surprising it was expected that the accuracy score was this low because of the way the data was situated on the scatterplot. This demonstrates that clustering algorithms are not efficient on data that are closely gathered.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The final algorithm used in this project was neural networks. Grid search was not performed so different parameters had to be manually adjusted including activation functions, layer weight initializers, kernel regularizes, and loss function. Activation functions were chosen from relu, sigmoid, softmax, softplus, and softsign. Layer weight initializers were chosen from he_normal, he_uniform, glorot_normal, and glorot_unfiorm. kernel regularizers include l1, l2, and l1_l2. Finally, when compiling the network, the loss functions used are chosen from sparse_categorical_crossentropy and binary_crossentropy. First, the model was created using the sequential API. Next, a flattened layer is added to the model with a shape of (none, 62) as this is the shape of the input. For the hidden layers, it was tested with two and three layers and I decided to keep three layers as the results indicated no noticeable difference. All three dense layers had an activation function of relu, kernel initializer of glorot uniform, and kernel regularizer of l2 after dozens of attempts to find the best parameters. Additionally, there is no noticeable difference between sparse_categorical and binary entropy loss function so sparse_categorical was chosen as the loss function. Neural networks take arrays as inputs so the X inputs and y outputs needed to be transformed into arrays. The result of the training and validation with the validation set showed that the validation loss had an exponential decrease until the 10th epoch and maintained a loss around 0.35 and the validation accuracy was maintained at 0.851. When testing the model with the X_valid set, the final accuracy was 0.85161494116718. This result was not impressive out of all the algorithms used in the project being outperformed by SVC and SGD classifier. The results from the classification report indicate that neural networks are a good algorithm for this project but as shown from previous algorithms, it isn’t the best for this machine learning problem.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Out of all algorithms experimented in the project, the one that outperformed was the SVC with the rbf kernel. Therefore, this model was chosen for a final evaluation and as expected, this evaluation exceeded all previous models selected. Compared to the rbf model during training, the precision, recall, and f1_score for true values were higher by 0.01. The accuracy score of 0.8576573161485974 was at least 0.01 higher than the accuracy score of most of the algorithms. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In conclusion, the machine learning problem being addressed in this project required 15 features after data preprocessing to predict whether there will be rain tomorrow. 50,000 instances of data were used to train 9 different algorithms and 12,000 instances were used to validate each model trained. The amount of features available raised the question of whether there are too many and if there any irrelevant features and the results of the experiments indicate that most of the features were relevant to the training of the algorithms. Through data processing, I learned how irregular real world data is and how many algorithms have similar results while others perform worse. However, the irregularity of the data created opportunities to experiment on the hyperparameters of each algorithm to create the most optimal model for training. These experiments are a rich experience allowing me to have a more in depth learning of each algorithm. Overall, this machine learning project taught me the importance of an optimal weather predictor and appreciate the hard work of computer scientists who develop machine learning models.
